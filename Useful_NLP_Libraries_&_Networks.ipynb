{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: Compare and contrast NLTK and spaCy in terms of features, ease of use, and performance.\n",
        "\n",
        "Answer:NLTK is a research-oriented NLP library that provides extensive tools for text processing, linguistics research, and education. It offers fine-grained control but requires more manual coding. spaCy is an industrial-strength NLP library focused on speed, ease of use, and production readiness, offering pretrained pipelines for tokenization, parsing, and named entity recognition. NLTK is better for learning and experimentation, while spaCy is preferred for real-world applications due to higher performance and optimized pipelines.\n",
        "\n",
        "Question 2: What is TextBlob and how does it simplify common NLP tasks like sentiment analysis and translation?\n",
        "\n",
        "Answer:TextBlob is a high-level NLP library built on top of NLTK and Pattern that simplifies common NLP tasks such as sentiment analysis, part-of-speech tagging, noun phrase extraction, and translation. It provides an easy-to-use API that allows developers to perform complex NLP operations with minimal code.\n",
        "\n",
        "Question 3: Explain the role of Stanford NLP in academic and industry NLP projects.\n",
        "\n",
        "Answer:Stanford NLP provides state-of-the-art tools such as CoreNLP that support tasks like parsing, named entity recognition, sentiment analysis, and dependency parsing. It is widely used in academic research for its linguistic accuracy and in industry for building robust NLP systems, especially when deep linguistic analysis is required.\n",
        "\n",
        "Question 4: Describe the architecture and functioning of a Recurrent Neural Network (RNN).\n",
        "\n",
        "Answer:A Recurrent Neural Network is a neural network designed to process sequential data by maintaining a hidden state that captures information from previous time steps. At each step, the network takes the current input and the previous hidden state to produce a new hidden state. This structure allows RNNs to model temporal dependencies in text, speech, and time-series data.\n",
        "\n",
        "Question 5: What is the key difference between LSTM and GRU networks in NLP applications?\n",
        "\n",
        "Answer:LSTM networks use three gates (input, forget, and output) and a separate cell state to capture long-term dependencies, while GRU networks use two gates (update and reset) and combine cell and hidden states. GRUs are simpler and faster to train, whereas LSTMs are more expressive and effective for very long sequences."
      ],
      "metadata": {
        "id": "9qLMhTAl3DkI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7_Ent_S2Xbz",
        "outputId": "64b7ee56-2432-4daf-d6f2-a3789dd9e15a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity: 0.21742424242424244\n",
            "Subjectivity: 0.6511363636363636\n"
          ]
        }
      ],
      "source": [
        "#Question 6: Sentiment analysis using TextBlob\n",
        "\n",
        "# Install TextBlob\n",
        "!pip install -q textblob\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "text = \"\"\"I had a great experience using the new mobile banking app.\n",
        "The interface is intuitive, and customer support was quick to resolve my issue.\n",
        "However, the app did crash once during a transaction, which was frustrating.\"\"\"\n",
        "\n",
        "blob = TextBlob(text)\n",
        "print(\"Polarity:\", blob.sentiment.polarity)\n",
        "print(\"Subjectivity:\", blob.sentiment.subjectivity)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Tokenization and frequency distribution using NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Required downloads for Python 3.12+\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines\n",
        "linguistics, computer science, and artificial intelligence. It enables machines\n",
        "to understand, interpret, and generate human language. Applications of NLP\n",
        "include chatbots, sentiment analysis, and machine translation.\"\"\"\n",
        "\n",
        "tokens = word_tokenize(text.lower())\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "print(freq_dist.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJF9_nYi3XwJ",
        "outputId": "5c577f2c-c317-4bea-ea55-f6744054516f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 6), ('and', 3), ('.', 3), ('language', 2), ('nlp', 2), ('natural', 1), ('processing', 1), ('(', 1), (')', 1), ('is', 1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Basic LSTM text classification using Keras\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Dataset\n",
        "texts = [\n",
        "    \"I love this project\",\n",
        "    \"This is an amazing experience\",\n",
        "    \"I hate waiting in line\",\n",
        "    \"This is the worst service\",\n",
        "    \"Absolutely fantastic\"\n",
        "]\n",
        "\n",
        "labels = np.array([1, 1, 0, 0, 1], dtype=\"float32\")\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded = pad_sequences(sequences, padding=\"post\")\n",
        "\n",
        "# Convert input to tensor (Keras 3 requirement)\n",
        "X = tf.convert_to_tensor(padded, dtype=tf.int32)\n",
        "\n",
        "# Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=8),\n",
        "    LSTM(16),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Train\n",
        "model.fit(X, labels, epochs=10, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdHnbgId3aEc",
        "outputId": "93ddffb2-907c-42db-a9c5-b509fe8b8f77"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.6000 - loss: 0.6927\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6000 - loss: 0.6913\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6000 - loss: 0.6900\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6000 - loss: 0.6886\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6000 - loss: 0.6872\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6000 - loss: 0.6857\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6000 - loss: 0.6843\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6000 - loss: 0.6829\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6000 - loss: 0.6814\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6000 - loss: 0.6798\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7aa77aa3ccb0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: spaCy pipeline for tokenization, lemmatization, and NER\n",
        "\n",
        "# Install spaCy and model\n",
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"\"\"Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role\n",
        "in the development of India’s atomic energy program. He was the founding director\n",
        "of the Tata Institute of Fundamental Research and helped establish the Atomic\n",
        "Energy Commission of India.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Tokens and Lemmas:\")\n",
        "for token in doc:\n",
        "    print(token.text, \"->\", token.lemma_)\n",
        "\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \"-\", ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgJ07HOh3hAq",
        "outputId": "c68c2d5d-423e-40d8-9d52-a999b28eb0f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Tokens and Lemmas:\n",
            "Homi -> Homi\n",
            "Jehangir -> Jehangir\n",
            "Bhaba -> Bhaba\n",
            "was -> be\n",
            "an -> an\n",
            "Indian -> indian\n",
            "nuclear -> nuclear\n",
            "physicist -> physicist\n",
            "who -> who\n",
            "played -> play\n",
            "a -> a\n",
            "key -> key\n",
            "role -> role\n",
            "\n",
            " -> \n",
            "\n",
            "in -> in\n",
            "the -> the\n",
            "development -> development\n",
            "of -> of\n",
            "India -> India\n",
            "’s -> ’s\n",
            "atomic -> atomic\n",
            "energy -> energy\n",
            "program -> program\n",
            ". -> .\n",
            "He -> he\n",
            "was -> be\n",
            "the -> the\n",
            "founding -> found\n",
            "director -> director\n",
            "\n",
            " -> \n",
            "\n",
            "of -> of\n",
            "the -> the\n",
            "Tata -> Tata\n",
            "Institute -> Institute\n",
            "of -> of\n",
            "Fundamental -> Fundamental\n",
            "Research -> Research\n",
            "and -> and\n",
            "helped -> help\n",
            "establish -> establish\n",
            "the -> the\n",
            "Atomic -> Atomic\n",
            "\n",
            " -> \n",
            "\n",
            "Energy -> Energy\n",
            "Commission -> Commission\n",
            "of -> of\n",
            "India -> India\n",
            ". -> .\n",
            "\n",
            "Named Entities:\n",
            "Homi Jehangir Bhaba - FAC\n",
            "Indian - NORP\n",
            "India - GPE\n",
            "the Tata Institute of Fundamental Research - ORG\n",
            "the Atomic\n",
            "Energy Commission of India - ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: NLP-based chatbot for a mental health platform\n",
        "\n",
        "Answer:I would preprocess user input using spaCy for tokenization, lemmatization, and entity recognition to extract intent and emotional cues. An LSTM or GRU model would be trained on labeled conversational data to capture contextual dependencies in user messages. The model architecture would include embeddings, recurrent layers, and a classification or response generation layer. Ethical considerations would include data privacy, bias mitigation, anonymization of user data, and ensuring the chatbot does not provide harmful or misleading advice, instead escalating critical cases to human professionals."
      ],
      "metadata": {
        "id": "SZdsVcU53ErY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3You9Sh3vJ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}